{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMswwL3MkJMMw84PsJ7LBDv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganeshmukhiya/Topic-Modeling-NMF/blob/main/Processed_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hfk5uNtCYS1s"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data files (only needed once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCUsW4zDYbcT",
        "outputId": "d3e6826a-7521-4baf-fc5d-d6a3a2d72a0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the training data\n",
        "train_data = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "id": "2v0GsSUbYiso"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the stop words and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "TsDawGYQYq5j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text data: lowercasing, tokenization, stop word removal, and stemming\n",
        "def preprocess(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words and apply stemming\n",
        "    words = [stemmer.stem(word) for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "ZOvoBQWnYzKq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to the entire dataset\n",
        "preprocessed_data = [preprocess(doc) for doc in train_data.data]"
      ],
      "metadata": {
        "id": "VSzoa3HeY4Fh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a sample of the preprocessed text data\n",
        "print(\"Original Text Example:\")\n",
        "print(train_data.data[0][:500])  # Print the first 500 characters of the original text\n",
        "\n",
        "print(\"\\nPreprocessed Text Example:\")\n",
        "print(preprocessed_data[0][:500])  # Print the first 500 characters of the preprocessed text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IIh8MofY9KU",
        "outputId": "61549280-348d-4220-9275-56073860b21a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text Example:\n",
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Preprocessed Text Example:\n",
            "wonder anyon could enlighten car saw day sport car look late earli 70 call bricklin door realli small addit front bumper separ rest bodi know anyon tellm model name engin spec year product car made histori whatev info funki look car pleas\n"
          ]
        }
      ]
    }
  ]
}