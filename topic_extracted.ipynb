{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9EEuFH2xooELcE1ED2H1H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganeshmukhiya/Topic-Modeling-NMF/blob/main/topic_extracted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Heq_OPPzhbO0",
        "outputId": "436d051c-7cf1-40db-d45d-8d1086f3ee6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic #1: don\n",
            "Topic #2: windows\n",
            "Topic #3: god\n",
            "Topic #4: geb\n",
            "Topic #5: key\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Fetch the data\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Vectorize the data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Set the number of topics\n",
        "n_topics = 5\n",
        "\n",
        "# Apply NMF\n",
        "nmf = NMF(n_components=n_topics, random_state=1)\n",
        "W_train = nmf.fit_transform(X_train)\n",
        "H = nmf.components_\n",
        "\n",
        "# Extract and display the latent topics with unique top words\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Set to track the words that have already been used\n",
        "used_words = set()\n",
        "\n",
        "for topic_idx, topic in enumerate(H):\n",
        "    # Sort terms by their importance to the topic\n",
        "    sorted_indices = topic.argsort()[::-1]\n",
        "\n",
        "    # Find the first term that hasn't been used yet\n",
        "    top_term = None\n",
        "    for index in sorted_indices:\n",
        "        word = feature_names[index]\n",
        "        if word not in used_words:\n",
        "            top_term = word\n",
        "            used_words.add(word)\n",
        "            break\n",
        "\n",
        "    print(f\"Topic #{topic_idx+1}: {top_term}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Fetch the data\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Vectorize the data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Set the number of topics\n",
        "n_topics = 5\n",
        "\n",
        "# Apply NMF\n",
        "nmf = NMF(n_components=n_topics, random_state=1)\n",
        "W_train = nmf.fit_transform(X_train)\n",
        "H = nmf.components_\n",
        "\n",
        "# Extract and display the latent topics with additional words associated with \"god\"\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Set to track the words that have already been used\n",
        "used_words = set()\n",
        "\n",
        "for topic_idx, topic in enumerate(H):\n",
        "    # Sort terms by their importance to the topic\n",
        "    sorted_indices = topic.argsort()[::-1]\n",
        "\n",
        "    # Find the top term that hasn't been used yet\n",
        "    top_term = None\n",
        "    for index in sorted_indices:\n",
        "        word = feature_names[index]\n",
        "        if word not in used_words:\n",
        "            top_term = word\n",
        "            used_words.add(word)\n",
        "            break\n",
        "\n",
        "    # Display the topic and additional words if the top word is \"god\"\n",
        "    if top_term == \"god\":\n",
        "        additional_words = [feature_names[i] for i in sorted_indices[1:6]]  # Next top 5 terms\n",
        "        print(f\"Topic #{topic_idx+1}: {top_term} (Additional words: {', '.join(additional_words)})\")\n",
        "    else:\n",
        "        print(f\"Topic #{topic_idx+1}: {top_term}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbbESniom0bK",
        "outputId": "ed464b31-231f-4385-b67b-616a8e2ab9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic #1: don\n",
            "Topic #2: windows\n",
            "Topic #3: god (Additional words: jesus, bible, believe, faith, christian)\n",
            "Topic #4: geb\n",
            "Topic #5: key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.utils import simple_preprocess\n",
        "import gensim\n",
        "\n",
        "# 1. Fetch the data\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "documents = newsgroups.data\n",
        "\n",
        "# 2. Preprocess the text to get bigrams\n",
        "def preprocess_text(texts):\n",
        "    # Use gensim's simple_preprocess for tokenization\n",
        "    return [simple_preprocess(doc, deacc=True) for doc in texts]\n",
        "\n",
        "processed_docs = preprocess_text(documents)\n",
        "\n",
        "# 3. Create bigrams using gensim's Phrases model\n",
        "bigram = gensim.models.Phrases(processed_docs, min_count=5, threshold=100)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "bigram_docs = [bigram_mod[doc] for doc in processed_docs]\n",
        "\n",
        "# 4. Create dictionary and corpus needed for coherence model\n",
        "dictionary = Dictionary(bigram_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in bigram_docs]\n",
        "\n",
        "# 5. Convert the text back into a format suitable for TF-IDF (space-separated bigrams)\n",
        "bigram_texts = [\" \".join(doc) for doc in bigram_docs]\n",
        "\n",
        "# 6. Apply TF-IDF Vectorizer with bigrams\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=2, stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(bigram_texts)\n",
        "\n",
        "# 7. Apply NMF for topic modeling\n",
        "num_topics = 10  # Define the number of topics\n",
        "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
        "W = nmf_model.fit_transform(X_tfidf)\n",
        "H = nmf_model.components_\n",
        "\n",
        "# 8. Display topics with top words (bi-grams)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(H):\n",
        "    print(f\"Topic #{topic_idx+1}:\")\n",
        "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-11:-1]]))  # Top 10 words\n",
        "\n",
        "# 9. Evaluate topic coherence using Gensim\n",
        "def format_topics_nmf(H, feature_names, num_top_words):\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(H):\n",
        "        top_words = [feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]\n",
        "        topics.append(top_words)\n",
        "    return topics\n",
        "\n",
        "topics = format_topics_nmf(H, feature_names, 10)\n",
        "\n",
        "# Convert NMF topics into Gensim-compatible format for coherence\n",
        "topics_gensim = [[word for word in topic] for topic in topics]\n",
        "\n",
        "# Coherence Model\n",
        "coherence_model_nmf = CoherenceModel(topics=topics_gensim, texts=bigram_docs, dictionary=dictionary, coherence='c_v')\n",
        "coherence_nmf = coherence_model_nmf.get_coherence()\n",
        "print(f\"Coherence Score: {coherence_nmf}\")\n"
      ],
      "metadata": {
        "id": "HpFmRDLPMQyh",
        "outputId": "fadb2b6b-cd80-4e7b-d775-40b5d5d19ab3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic #1:\n",
            "just don like think good ve car know time really\n",
            "Topic #2:\n",
            "windows dos ms file os mouse problem run running drivers\n",
            "Topic #3:\n",
            "intellect geb_cadre edu shameful geb_cadre dsl_pitt geb_cadre shameful surrender dsl_pitt edu dsl_pitt surrender soon chastity intellect jxp_skepticism chastity\n",
            "Topic #4:\n",
            "god jesus bible believe faith christ christian christians sin say\n",
            "Topic #5:\n",
            "game team games year hockey baseball players season espn play\n",
            "Topic #6:\n",
            "drive scsi mb card ide mhz drives mac disk bit\n",
            "Topic #7:\n",
            "thanks does know advance thanks advance does know mail hi anybody info\n",
            "Topic #8:\n",
            "file program use software image available data edu graphics files\n",
            "Topic #9:\n",
            "people government israel jews state said did right children israeli\n",
            "Topic #10:\n",
            "key keys chip encryption government clipper use algorithm bit des\n",
            "Coherence Score: 0.7375409049513133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and display the latent topics where the top term is a two-word bi-gram\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Set to track the words that have already been used\n",
        "used_words = set()\n",
        "\n",
        "for topic_idx, topic in enumerate(H):\n",
        "    # Sort terms by their importance to the topic\n",
        "    sorted_indices = topic.argsort()[::-1]\n",
        "\n",
        "    # Find the top term that is a two-word bi-gram and hasn't been used yet\n",
        "    top_term = None\n",
        "    for index in sorted_indices:\n",
        "        word = feature_names[index]\n",
        "        if len(word.split()) == 2 and word not in used_words:  # Check if the term is a bi-gram (2 words)\n",
        "            top_term = word\n",
        "            used_words.add(word)\n",
        "            break\n",
        "\n",
        "    # Display the bi-gram topic and additional words\n",
        "    if top_term:\n",
        "        additional_words = [feature_names[i] for i in sorted_indices[1:6]]  # Next top 5 terms\n",
        "        print(f\"Topic #{topic_idx+1}: {top_term} (Additional words: {', '.join(additional_words)})\")\n"
      ],
      "metadata": {
        "id": "o8T2nq7QOAvN",
        "outputId": "81347fcb-d09f-4c9a-f09f-c5c8f1a4615b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic #1: don know (Additional words: don, like, think, good, ve)\n",
            "Topic #2: ms windows (Additional words: dos, ms, file, os, mouse)\n",
            "Topic #3: intellect geb_cadre (Additional words: edu shameful, geb_cadre dsl_pitt, geb_cadre, shameful surrender, dsl_pitt edu)\n",
            "Topic #4: believe god (Additional words: jesus, bible, believe, faith, christ)\n",
            "Topic #5: baseball game (Additional words: team, games, year, hockey, baseball)\n",
            "Topic #6: ide drive (Additional words: scsi, mb, card, ide, mhz)\n",
            "Topic #7: thanks advance (Additional words: does, know, advance, thanks advance, does know)\n",
            "Topic #8: computer graphics (Additional words: program, use, software, image, available)\n",
            "Topic #9: right people (Additional words: government, israel, jews, state, said)\n",
            "Topic #10: public key (Additional words: keys, chip, encryption, government, clipper)\n"
          ]
        }
      ]
    }
  ]
}